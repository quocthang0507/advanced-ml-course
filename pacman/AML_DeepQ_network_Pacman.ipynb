{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQ7K64H0Xl-i"
   },
   "outputs": [],
   "source": [
    "!pip install gymnasium[atari,accept-rom-license] ale-py numpy tensorflow matplotlib opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5-mJ94JXs7W"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import ale_py\n",
    "from collections import deque\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "# tf.config.experimental.set_memory_growth(tf.config.experimental.list_physical_devices('GPU')[0], True)  # Enable memory growth for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5-dKpHD3XwwC"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "GAMMA = 0.99  # Discount factor\n",
    "EPSILON = 1.0  # Initial exploration rate\n",
    "EPSILON_MIN = 0.1  # Minimum exploration rate\n",
    "EPSILON_DECAY = 0.995  # Decay rate\n",
    "LEARNING_RATE = 0.00025  # Learning rate\n",
    "MEMORY_SIZE = 5000  # Experience replay buffer size\n",
    "BATCH_SIZE = 64  # Batch size\n",
    "TARGET_UPDATE_FREQ = 10  # Target model update frequency\n",
    "EPISODES = 50000  # Total training episodes\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ToW7zUVuXyhj"
   },
   "outputs": [],
   "source": [
    "# Create Pac-Man environment\n",
    "env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\")\n",
    "state_shape = (88, 80, 1)  # Resized grayscale shape\n",
    "# action_size = env.action_space.n\n",
    "action_size = 5 # nope, up, right, down, left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xhvJd3W0X0hb"
   },
   "outputs": [],
   "source": [
    "# Function to preprocess frames\n",
    "def preprocess_state(state):\n",
    "    \"\"\"Convert RGB to grayscale and resize.\"\"\"\n",
    "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "    state = cv2.resize(state, (80, 88))  # Resize\n",
    "    return np.expand_dims(state, axis=-1) / 255.0  # Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J8Y2P73OX2ve"
   },
   "outputs": [],
   "source": [
    "# Build the DQN model with GPU optimization\n",
    "def build_model():\n",
    "    model = Sequential([\n",
    "        Conv2D(32, (8, 8), strides=(4, 4), activation=\"relu\", input_shape=state_shape),\n",
    "        Conv2D(64, (4, 4), strides=(2, 2), activation=\"relu\"),\n",
    "        Conv2D(64, (3, 3), strides=(1, 1), activation=\"relu\"),\n",
    "        Flatten(),\n",
    "        Dense(512, activation=\"relu\"),\n",
    "        Dense(action_size, activation=\"linear\")  # Q-values output\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=\"mse\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKD8jE1xX5wH"
   },
   "outputs": [],
   "source": [
    "# DQN Agent\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.model = build_model()\n",
    "        self.target_model = build_model()\n",
    "        self.target_model.set_weights(self.model.get_weights())  # Sync target model\n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
    "        self.epsilon = EPSILON\n",
    "        self.state_memory = np.zeros((MEMORY_SIZE, *state_shape), dtype=np.float32)\n",
    "        self.next_state_memory = np.zeros((MEMORY_SIZE, *state_shape), dtype=np.float32)\n",
    "        self.action_memory = np.zeros(MEMORY_SIZE, dtype=np.int32)\n",
    "        self.reward_memory = np.zeros(MEMORY_SIZE, dtype=np.float32)\n",
    "        self.done_memory = np.zeros(MEMORY_SIZE, dtype=np.bool)\n",
    "        self.memory_counter = 0\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choose action using Îµ-greedy strategy.\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(action_size)  # Random action (exploration)\n",
    "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
    "        return np.argmax(q_values[0])  # Best action (exploitation)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store experience in memory.\"\"\"\n",
    "        index = self.memory_counter % MEMORY_SIZE\n",
    "        self.state_memory[index] = state\n",
    "        self.next_state_memory[index] = next_state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.done_memory[index] = done\n",
    "        self.memory_counter += 1\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Train the model using experience replay.\"\"\"\n",
    "        if self.memory_counter < BATCH_SIZE:\n",
    "            return\n",
    "        max_mem = min(self.memory_counter, MEMORY_SIZE)\n",
    "        batch_indices = np.random.choice(max_mem, BATCH_SIZE, replace=False)\n",
    "        states = self.state_memory[batch_indices]\n",
    "        next_states = self.next_state_memory[batch_indices]\n",
    "        actions = self.action_memory[batch_indices]\n",
    "        rewards = self.reward_memory[batch_indices]\n",
    "        dones = self.done_memory[batch_indices]\n",
    "\n",
    "        targets = self.model.predict(states, verbose=0)\n",
    "        next_q_values = self.target_model.predict(next_states, verbose=0)\n",
    "        for i in range(BATCH_SIZE):\n",
    "            if dones[i]:\n",
    "                targets[i, actions[i]] = rewards[i]\n",
    "            else:\n",
    "                targets[i, actions[i]] = rewards[i] + GAMMA * np.max(next_q_values[i])\n",
    "\n",
    "        # Train model in batches\n",
    "        self.model.fit(states, targets, epochs=1, verbose=0, batch_size=BATCH_SIZE)\n",
    "\n",
    "        if self.epsilon > EPSILON_MIN:\n",
    "            self.epsilon *= EPSILON_DECAY  # Decay exploration rate\n",
    "\n",
    "    def update_target_model(self):\n",
    "        \"\"\"Update target model weights.\"\"\"\n",
    "        self.target_model.set_weights(self.model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hzhffBoGX6pZ"
   },
   "outputs": [],
   "source": [
    "# Train the agent\n",
    "agent = DQNAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZbeKtAlqX9tj"
   },
   "outputs": [],
   "source": [
    "# Visualization function for Colab\n",
    "def show_frame(frame):\n",
    "    plt.imshow(frame)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zVtEPbKzYBQT"
   },
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train():\n",
    "    with open('training_log.txt', 'w') as f:\n",
    "        f.write(\"Episode\\tScore\\tEpsilon\\n\")\n",
    "        for episode in range(EPISODES):\n",
    "            state = preprocess_state(env.reset()[0])\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            while not done:\n",
    "                action = agent.act(state)\n",
    "                next_state, reward, done, _, _ = env.step(action)\n",
    "                next_state = preprocess_state(next_state)\n",
    "                agent.remember(state, action, reward, next_state, done)\n",
    "                state = next_state\n",
    "                total_reward += reward\n",
    "                agent.replay()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % TARGET_UPDATE_FREQ == 0:\n",
    "                agent.update_target_model()\n",
    "\n",
    "            log_print = f\"{episode + 1}/{EPISODES}\\t{total_reward}\\t{agent.epsilon:.4f}\\n\"\n",
    "            log = f\"{episode + 1}\\t{total_reward}\\t{agent.epsilon:.4f}\\n\"\n",
    "            print(log_print)\n",
    "            f.write(log)\n",
    "\n",
    "        env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1yfp_xt3hme9"
   },
   "outputs": [],
   "source": [
    "if LOAD_MODEL:\n",
    "    agent.model = load_model(\"pacman_dqn.keras\")\n",
    "else:\n",
    "    train()\n",
    "    agent.model.save(\"pacman_dqn.keras\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
