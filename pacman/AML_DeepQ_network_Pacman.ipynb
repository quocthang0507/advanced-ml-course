{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari,accept-rom-license]\n",
        "!pip install ale-py"
      ],
      "metadata": {
        "id": "UQ7K64H0Xl-i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "import ale_py\n",
        "from collections import deque\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "e5-mJ94JXs7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "GAMMA = 0.99  # Discount factor\n",
        "EPSILON = 1.0  # Initial exploration rate\n",
        "EPSILON_MIN = 0.1  # Minimum exploration rate\n",
        "EPSILON_DECAY = 0.995  # Decay rate\n",
        "LEARNING_RATE = 0.00025  # Learning rate\n",
        "MEMORY_SIZE = 5000  # Experience replay buffer size\n",
        "BATCH_SIZE = 64  # Larger batch size for better GPU utilization\n",
        "TARGET_UPDATE_FREQ = 10  # Target model update frequency\n",
        "EPISODES = 500  # Total training episodes\n",
        "RECORD = False\n",
        "LOAD_MODEL = False"
      ],
      "metadata": {
        "id": "5-dKpHD3XwwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Pac-Man environment\n",
        "env = gym.make(\"ALE/MsPacman-v5\", render_mode=\"rgb_array\")\n",
        "state_shape = (88, 80, 1)  # Resized grayscale shape\n",
        "action_size = env.action_space.n"
      ],
      "metadata": {
        "id": "ToW7zUVuXyhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess frames\n",
        "def preprocess_state(state):\n",
        "    \"\"\"Convert RGB to grayscale and resize.\"\"\"\n",
        "    state = cv2.cvtColor(state, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
        "    state = cv2.resize(state, (80, 88))  # Resize\n",
        "    return np.expand_dims(state, axis=-1) / 255.0  # Normalize"
      ],
      "metadata": {
        "id": "xhvJd3W0X0hb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the DQN model with GPU optimization\n",
        "def build_model():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (8, 8), strides=(4, 4), activation=\"relu\", input_shape=state_shape),\n",
        "        Conv2D(64, (4, 4), strides=(2, 2), activation=\"relu\"),\n",
        "        Conv2D(64, (3, 3), strides=(1, 1), activation=\"relu\"),\n",
        "        Flatten(),\n",
        "        Dense(512, activation=\"relu\"),\n",
        "        Dense(action_size, activation=\"linear\")  # Q-values output\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=LEARNING_RATE), loss=\"mse\")\n",
        "    return model"
      ],
      "metadata": {
        "id": "J8Y2P73OX2ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DQN Agent\n",
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.model = build_model()\n",
        "        self.target_model = build_model()\n",
        "        self.target_model.set_weights(self.model.get_weights())  # Sync target model\n",
        "        self.memory = deque(maxlen=MEMORY_SIZE)\n",
        "        self.epsilon = EPSILON\n",
        "\n",
        "    def act(self, state):\n",
        "        \"\"\"Choose action using Îµ-greedy strategy.\"\"\"\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return random.randrange(action_size)  # Random action (exploration)\n",
        "        q_values = self.model.predict(np.expand_dims(state, axis=0), verbose=0)\n",
        "        return np.argmax(q_values[0])  # Best action (exploitation)\n",
        "\n",
        "    def remember(self, state, action, reward, next_state, done):\n",
        "        \"\"\"Store experience in memory.\"\"\"\n",
        "        self.memory.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def replay(self):\n",
        "        \"\"\"Train the model using experience replay.\"\"\"\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)\n",
        "        states, targets = [], []\n",
        "        for state, action, reward, next_state, done in batch:\n",
        "            target = self.model.predict(np.expand_dims(state, axis=0), verbose=0)[0]\n",
        "            if done:\n",
        "                target[action] = reward\n",
        "            else:\n",
        "                next_q_values = self.target_model.predict(np.expand_dims(next_state, axis=0), verbose=0)[0]\n",
        "                target[action] = reward + GAMMA * np.max(next_q_values)\n",
        "            states.append(state)\n",
        "            targets.append(target)\n",
        "\n",
        "        # Train model in batches (GPU optimized)\n",
        "        self.model.fit(np.array(states), np.array(targets), epochs=1, verbose=0, batch_size=BATCH_SIZE)\n",
        "\n",
        "        if self.epsilon > EPSILON_MIN:\n",
        "            self.epsilon *= EPSILON_DECAY  # Decay exploration rate\n",
        "\n",
        "    def update_target_model(self):\n",
        "        \"\"\"Update target model weights.\"\"\"\n",
        "        self.target_model.set_weights(self.model.get_weights())"
      ],
      "metadata": {
        "id": "FKD8jE1xX5wH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent\n",
        "agent = DQNAgent()"
      ],
      "metadata": {
        "id": "hzhffBoGX6pZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization function for Colab\n",
        "def show_frame(frame):\n",
        "    plt.imshow(frame)\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ZbeKtAlqX9tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "def train():\n",
        "    for episode in range(EPISODES):\n",
        "        state = preprocess_state(env.reset()[0])\n",
        "        done = False\n",
        "        total_reward = 0\n",
        "\n",
        "        while not done:\n",
        "            action = agent.act(state)\n",
        "            next_state, reward, done, _, _ = env.step(action)\n",
        "            next_state = preprocess_state(next_state)\n",
        "            agent.remember(state, action, reward, next_state, done)\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            agent.replay()\n",
        "\n",
        "        # Update target network periodically\n",
        "        if episode % TARGET_UPDATE_FREQ == 0:\n",
        "            agent.update_target_model()\n",
        "\n",
        "        print(f\"Episode {episode + 1}/{EPISODES}, Score: {total_reward}, Epsilon: {agent.epsilon:.4f}\")\n",
        "\n",
        "    env.close()"
      ],
      "metadata": {
        "id": "zVtEPbKzYBQT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if LOAD_MODEL:\n",
        "    agent.model = load_model(\"pacman_dqn_gpu.h5\")\n",
        "else:\n",
        "    train()\n",
        "    agent.model.save(\"pacman_dqn_gpu.h5\")"
      ],
      "metadata": {
        "id": "1yfp_xt3hme9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "8d9afed10e14615d8b999dd99cb7fb11186cc4dcfcde1b40cb5785c433d361e4"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}